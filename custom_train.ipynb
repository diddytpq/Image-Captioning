{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "638a2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer, ViTImageProcessor\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c74904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bebb5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset make_coco_train_data/2017 to C:/Users/DRCL/.cache/huggingface/datasets/make_coco_train_data/2017-6486e79d16f782ce/0.0.0/102355ff62a645404d15fc83bbaa46137b14eeb4c8396d78238d39d0a8775fe4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset make_coco_train_data downloaded and prepared to C:/Users/DRCL/.cache/huggingface/datasets/make_coco_train_data/2017-6486e79d16f782ce/0.0.0/102355ff62a645404d15fc83bbaa46137b14eeb4c8396d78238d39d0a8775fe4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 186.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path = os.getcwd() + \"/coco2017_data/\"\n",
    "# ds = datasets.load_dataset(path = \"make_coco_train_data\", name = \"2017\", data_dir=data_path)\n",
    "\n",
    "data_path = \"D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/\"\n",
    "ds = datasets.load_dataset(path = \"make_coco_train_data\", name = \"2017\", data_dir=data_path)\n",
    "\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
    "        num_rows: 80\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
    "        num_rows: 80\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
    "        num_rows: 16\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f48d61d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 84\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [{\"image_id\" : -1, \n",
    "            \"caption_id\" : -1, \n",
    "            \"height\" : 720, \n",
    "            \"width\" : 1280, \n",
    "            \"file_name\" : \"test.png\", \n",
    "            \"coco_url\" : \"none\", \n",
    "            \"image_path\" : os.getcwd() + \"/images/test.png\",\n",
    "            \"caption\": \"The man is building a fire.\"},\n",
    "            {\"image_id\" : -1, \n",
    "            \"caption_id\" : -1, \n",
    "            \"height\" : 720, \n",
    "            \"width\" : 1280, \n",
    "            \"file_name\" : \"fire_00007690.jpg\", \n",
    "            \"coco_url\" : \"none\", \n",
    "            \"image_path\" : os.getcwd() + \"/images/fire_00007690.jpg\",\n",
    "            \"caption\": \"The pot is on fire.\"},\n",
    "            {\"image_id\" : -1, \n",
    "            \"caption_id\" : -1, \n",
    "            \"height\" : 720, \n",
    "            \"width\" : 1280, \n",
    "            \"file_name\" : \"fire_00007622.jpg\", \n",
    "            \"coco_url\" : \"none\", \n",
    "            \"image_path\" : os.getcwd() + \"/images/fire_00007622.jpg\",\n",
    "            \"caption\": \"A person is sitting next to a pot.\"},\n",
    "            {\"image_id\" : -1, \n",
    "            \"caption_id\" : -1, \n",
    "            \"height\" : 720, \n",
    "            \"width\" : 1280, \n",
    "            \"file_name\" : \"fire_00007628.jpg\", \n",
    "            \"coco_url\" : \"none\", \n",
    "            \"image_path\" : os.getcwd() + \"/images/fire_00007628.jpg\",\n",
    "            \"caption\": \"A person is standing next to a pot and the pot is on fire.\"}]\n",
    "new_ds = ds\n",
    "\n",
    "for data in new_data:\n",
    "    new_ds[\"train\"] = new_ds[\"train\"].add_item(data)\n",
    "\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df41fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000074.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000074.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000074.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000074.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000074.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000073.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000073.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000073.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000073.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000073.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000042.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000042.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000042.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000042.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000042.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000049.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000049.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000049.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000049.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000049.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000025.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000025.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000025.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000025.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000025.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000072.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000072.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000072.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000072.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000081.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000081.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000081.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000081.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000081.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000072.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000071.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000071.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000071.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000071.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000071.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000061.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000061.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000061.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000061.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000061.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000036.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000036.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000036.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000036.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000036.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000034.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000034.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000034.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000034.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000034.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000077.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000077.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000077.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000077.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000077.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000009.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000009.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000009.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000009.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000009.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000030.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000030.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000030.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000030.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000030.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000064.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000064.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000064.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000064.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000064.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000078.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000078.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000078.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000078.jpg\n",
      "D:\\yoseph\\DataSet\\Captioning_dataset/coco2017_data_test/train2017\\000000000078.jpg\n",
      "c:\\Users\\DRCL\\Desktop\\Image-Captioning/images/test.png\n",
      "c:\\Users\\DRCL\\Desktop\\Image-Captioning/images/fire_00007690.jpg\n",
      "c:\\Users\\DRCL\\Desktop\\Image-Captioning/images/fire_00007622.jpg\n",
      "c:\\Users\\DRCL\\Desktop\\Image-Captioning/images/fire_00007628.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in range(84):\n",
    "    print(new_ds['train'][i]['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8948ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29142399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/test-model\\\\tokenizer_config.json',\n",
       " './model/test-model\\\\special_tokens_map.json',\n",
       " './model/test-model\\\\vocab.json',\n",
       " './model/test-model\\\\merges.txt',\n",
       " './model/test-model\\\\added_tokens.json',\n",
       " './model/test-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./model/test-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b0e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, padding=\"max_length\", max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        to_keep = []\n",
    "        for image_file in image_paths:\n",
    "            try:\n",
    "                # img = Image.open(image_file)\n",
    "                img = cv2.imread(image_file)\n",
    "                if len(img.shape) == 2:  # 이미지가 2차원인 경우\n",
    "                    img = np.expand_dims(img, axis=2)  # 차원을 확장하여 3차원으로 만듦\n",
    "\n",
    "                images.append(img)\n",
    "                to_keep.append(True)\n",
    "            except Exception:\n",
    "                to_keep.append(False)\n",
    "    else:\n",
    "        images = [Image.open(image_file) for image_file in image_paths]\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image_path']\n",
    "    captions = examples['caption']    \n",
    "    \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5993e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "# processed_dataset = ds.map(function=preprocess_fn, batched=True, fn_kwargs={\"max_target_length\": 128}, remove_columns=ds['train'].column_names)\n",
    "processed_dataset = new_ds.map(function=preprocess_fn, batched=True, fn_kwargs={\"max_target_length\": 2000}, remove_columns=ds['train'].column_names)\n",
    "\n",
    "# processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcc0996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "processed_dataset.save_to_disk('./processed_dataset')\n",
    "\n",
    "test = processed_dataset.load_from_disk('./processed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5219bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(predict_with_generate=True, evaluation_strategy=\"epoch\", per_device_train_batch_size=4, per_device_eval_batch_size=4, output_dir=\"./model/image-captioning-output\",num_train_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddb678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb6bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DRCL\\Desktop\\Image-Captioning\\custom_train.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DRCL/Desktop/Image-Captioning/custom_train.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# instantiate trainer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DRCL/Desktop/Image-Captioning/custom_train.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mfeature_extractor, args\u001b[39m=\u001b[39mtraining_args, compute_metrics\u001b[39m=\u001b[39mcompute_metrics, train_dataset\u001b[39m=\u001b[39mprocessed_dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], eval_dataset\u001b[39m=\u001b[39mprocessed_dataset[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m], data_collator\u001b[39m=\u001b[39mdefault_data_collator,)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DRCL/Desktop/Image-Captioning/custom_train.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2698\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2699\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2702\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:609\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m    605\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    608\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m    610\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m    611\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m    612\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    613\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    614\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m    615\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    616\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    617\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    618\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    619\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    620\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_decoder,\n\u001b[0;32m    621\u001b[0m )\n\u001b[0;32m    623\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[0;32m    624\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1076\u001b[0m     input_ids,\n\u001b[0;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    900\u001b[0m         hidden_states,\n\u001b[0;32m    901\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    903\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    904\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    905\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    906\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    907\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    908\u001b[0m     )\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    387\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 389\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    390\u001b[0m     hidden_states,\n\u001b[0;32m    391\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    392\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    393\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    394\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    395\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    396\u001b[0m )\n\u001b[0;32m    397\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    398\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    332\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\DRCL\\anaconda3\\envs\\caption\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:200\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    197\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(attn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[0;32m    198\u001b[0m     \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull([], mask_value, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    201\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(causal_mask, attn_weights\u001b[39m.\u001b[39mto(attn_weights\u001b[39m.\u001b[39mdtype), mask_value)\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(model=model, tokenizer=feature_extractor, args=training_args, compute_metrics=compute_metrics, train_dataset=processed_dataset['train'], eval_dataset=processed_dataset['validation'], data_collator=default_data_collator,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec776333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./image-captioning-output\\\\tokenizer_config.json',\n",
       " './image-captioning-output\\\\special_tokens_map.json',\n",
       " './image-captioning-output\\\\vocab.json',\n",
       " './image-captioning-output\\\\merges.txt',\n",
       " './image-captioning-output\\\\added_tokens.json',\n",
       " './image-captioning-output\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./image-captioning-output\")\n",
    "tokenizer.save_pretrained(\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dab6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./image-captioning-output\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"./image-captioning-output\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./image-captioning-output\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 32\n",
    "num_beams = 12\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba78ae04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A giraffe standing next to a tree.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(['./images/Sunset_TB_360_00-00-03.jpg']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d08d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
